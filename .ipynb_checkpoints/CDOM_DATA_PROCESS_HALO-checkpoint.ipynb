{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5dcf0f10",
   "metadata": {},
   "source": [
    "# DRAFT OF THIS IS FOR GOM2021 ONLY\n",
    "Inputs: cdom files \n",
    "outputs: spectra.csv, outputs.csv, fig.csv\n",
    "        \n",
    "    Hanna Bridgham \n",
    "    last edited 07/31/2023\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ccdea05",
   "metadata": {},
   "source": [
    "## PART 1: QA/QC\n",
    "\n",
    "Loading data and checking for issues "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "82c8f1ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#LOAD REQUIERD PACKAGES \n",
    "import os # OS library \n",
    "import pandas as pd #the csv reading library \n",
    "import matplotlib.pyplot as plt #the ploting library \n",
    "import glob #to get file names \n",
    "\n",
    "from scipy.interpolate import PchipInterpolator # Interpolator \n",
    "import numpy as np \n",
    "from scipy.optimize import curve_fit \n",
    "from scipy.optimize import minimize \n",
    "import re\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb251f33",
   "metadata": {},
   "source": [
    "NOTES ON COMPLETED WORK:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7133fbbc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I have your files\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['../HALO/Data/CDOM/WC_Abs_HALO_2227\\\\222703_Absorbance_01.txt',\n",
       " '../HALO/Data/CDOM/WC_Abs_HALO_2227\\\\222703_Absorbance_02.txt',\n",
       " '../HALO/Data/CDOM/WC_Abs_HALO_2227\\\\222703_Absorbance_03.txt',\n",
       " '../HALO/Data/CDOM/WC_Abs_HALO_2227\\\\222704_Absorbance_01.txt',\n",
       " '../HALO/Data/CDOM/WC_Abs_HALO_2227\\\\222704_Absorbance_02.txt',\n",
       " '../HALO/Data/CDOM/WC_Abs_HALO_2227\\\\222704_Absorbance_03.txt',\n",
       " '../HALO/Data/CDOM/WC_Abs_HALO_2227\\\\222705_Absorbance_01.txt',\n",
       " '../HALO/Data/CDOM/WC_Abs_HALO_2227\\\\222705_Absorbance_02.txt',\n",
       " '../HALO/Data/CDOM/WC_Abs_HALO_2227\\\\222705_Absorbance_03.txt',\n",
       " '../HALO/Data/CDOM/WC_Abs_HALO_2227\\\\222706_Absorbance_01.txt',\n",
       " '../HALO/Data/CDOM/WC_Abs_HALO_2227\\\\222706_Absorbance_02.txt',\n",
       " '../HALO/Data/CDOM/WC_Abs_HALO_2227\\\\222706_Absorbance_03.txt',\n",
       " '../HALO/Data/CDOM/WC_Abs_HALO_2227\\\\222707_Absorbance_01.txt',\n",
       " '../HALO/Data/CDOM/WC_Abs_HALO_2227\\\\222707_Absorbance_02.txt',\n",
       " '../HALO/Data/CDOM/WC_Abs_HALO_2227\\\\222707_Absorbance_03.txt',\n",
       " '../HALO/Data/CDOM/WC_Abs_HALO_2227\\\\222708_Absorbance_01.txt',\n",
       " '../HALO/Data/CDOM/WC_Abs_HALO_2227\\\\222708_Absorbance_02.txt',\n",
       " '../HALO/Data/CDOM/WC_Abs_HALO_2227\\\\222708_Absorbance_03.txt',\n",
       " '../HALO/Data/CDOM/WC_Abs_HALO_2227\\\\222709_Absorbance_01.txt',\n",
       " '../HALO/Data/CDOM/WC_Abs_HALO_2227\\\\222709_Absorbance_02.txt',\n",
       " '../HALO/Data/CDOM/WC_Abs_HALO_2227\\\\222709_Absorbance_03.txt',\n",
       " '../HALO/Data/CDOM/WC_Abs_HALO_2227\\\\222710_Absorbance_01.txt',\n",
       " '../HALO/Data/CDOM/WC_Abs_HALO_2227\\\\222710_Absorbance_02.txt',\n",
       " '../HALO/Data/CDOM/WC_Abs_HALO_2227\\\\222710_Absorbance_03.txt',\n",
       " '../HALO/Data/CDOM/WC_Abs_HALO_2227\\\\222711_Absorbance_01.txt',\n",
       " '../HALO/Data/CDOM/WC_Abs_HALO_2227\\\\222711_Absorbance_02.txt',\n",
       " '../HALO/Data/CDOM/WC_Abs_HALO_2227\\\\222711_Absorbance_03.txt',\n",
       " '../HALO/Data/CDOM/WC_Abs_HALO_2227\\\\222711_Absorbance_04.txt',\n",
       " '../HALO/Data/CDOM/WC_Abs_HALO_2227\\\\222711_Absorbance_05.txt',\n",
       " '../HALO/Data/CDOM/WC_Abs_HALO_2227\\\\222711_Absorbance_06.txt',\n",
       " '../HALO/Data/CDOM/WC_Abs_HALO_2227\\\\222712_Absorbance_01.txt',\n",
       " '../HALO/Data/CDOM/WC_Abs_HALO_2227\\\\222712_Absorbance_02.txt',\n",
       " '../HALO/Data/CDOM/WC_Abs_HALO_2227\\\\222712_Absorbance_03.txt',\n",
       " '../HALO/Data/CDOM/WC_Abs_HALO_2227\\\\222713_Absorbance_01.txt',\n",
       " '../HALO/Data/CDOM/WC_Abs_HALO_2227\\\\222713_Absorbance_02.txt',\n",
       " '../HALO/Data/CDOM/WC_Abs_HALO_2227\\\\222713_Absorbance_03.txt',\n",
       " '../HALO/Data/CDOM/WC_Abs_HALO_2227\\\\222714_Absorbance_01.txt',\n",
       " '../HALO/Data/CDOM/WC_Abs_HALO_2227\\\\222714_Absorbance_02.txt',\n",
       " '../HALO/Data/CDOM/WC_Abs_HALO_2227\\\\222714_Absorbance_03.txt',\n",
       " '../HALO/Data/CDOM/WC_Abs_HALO_2227\\\\222715_Absorbance_01.txt',\n",
       " '../HALO/Data/CDOM/WC_Abs_HALO_2227\\\\222715_Absorbance_02.txt',\n",
       " '../HALO/Data/CDOM/WC_Abs_HALO_2227\\\\222715_Absorbance_03.txt',\n",
       " '../HALO/Data/CDOM/WC_Abs_HALO_2227\\\\222716_Absorbance_01.txt',\n",
       " '../HALO/Data/CDOM/WC_Abs_HALO_2227\\\\222716_Absorbance_02.txt',\n",
       " '../HALO/Data/CDOM/WC_Abs_HALO_2227\\\\222716_Absorbance_03.txt',\n",
       " '../HALO/Data/CDOM/WC_Abs_HALO_2227\\\\222727_Absorbance_01.txt',\n",
       " '../HALO/Data/CDOM/WC_Abs_HALO_2227\\\\222727_Absorbance_02.txt',\n",
       " '../HALO/Data/CDOM/WC_Abs_HALO_2227\\\\222727_Absorbance_03.txt',\n",
       " '../HALO/Data/CDOM/WC_Abs_HALO_2227\\\\222728_Absorbance_01.txt',\n",
       " '../HALO/Data/CDOM/WC_Abs_HALO_2227\\\\222728_Absorbance_02.txt',\n",
       " '../HALO/Data/CDOM/WC_Abs_HALO_2227\\\\222728_Absorbance_03.txt',\n",
       " '../HALO/Data/CDOM/WC_Abs_HALO_2227\\\\222729_Absorbance_01.txt',\n",
       " '../HALO/Data/CDOM/WC_Abs_HALO_2227\\\\222729_Absorbance_02.txt',\n",
       " '../HALO/Data/CDOM/WC_Abs_HALO_2227\\\\222729_Absorbance_03.txt',\n",
       " '../HALO/Data/CDOM/WC_Abs_HALO_2227\\\\222730_Absorbance_01.txt',\n",
       " '../HALO/Data/CDOM/WC_Abs_HALO_2227\\\\222730_Absorbance_02.txt',\n",
       " '../HALO/Data/CDOM/WC_Abs_HALO_2227\\\\222730_Absorbance_03.txt',\n",
       " '../HALO/Data/CDOM/WC_Abs_HALO_2227\\\\222731_Absorbance_01.txt',\n",
       " '../HALO/Data/CDOM/WC_Abs_HALO_2227\\\\222731_Absorbance_02.txt',\n",
       " '../HALO/Data/CDOM/WC_Abs_HALO_2227\\\\222731_Absorbance_03.txt',\n",
       " '../HALO/Data/CDOM/WC_Abs_HALO_2227\\\\222732_Absorbance_01.txt',\n",
       " '../HALO/Data/CDOM/WC_Abs_HALO_2227\\\\222732_Absorbance_02.txt',\n",
       " '../HALO/Data/CDOM/WC_Abs_HALO_2227\\\\222732_Absorbance_03.txt',\n",
       " '../HALO/Data/CDOM/WC_Abs_HALO_2227\\\\222733_Absorbance_01.txt',\n",
       " '../HALO/Data/CDOM/WC_Abs_HALO_2227\\\\222733_Absorbance_02.txt',\n",
       " '../HALO/Data/CDOM/WC_Abs_HALO_2227\\\\222733_Absorbance_03.txt',\n",
       " '../HALO/Data/CDOM/WC_Abs_HALO_2227\\\\222734_Absorbance_01.txt',\n",
       " '../HALO/Data/CDOM/WC_Abs_HALO_2227\\\\222734_Absorbance_02.txt',\n",
       " '../HALO/Data/CDOM/WC_Abs_HALO_2227\\\\222734_Absorbance_03.txt',\n",
       " '../HALO/Data/CDOM/WC_Abs_HALO_2227\\\\222735_Absorbance_01.txt',\n",
       " '../HALO/Data/CDOM/WC_Abs_HALO_2227\\\\222735_Absorbance_02.txt',\n",
       " '../HALO/Data/CDOM/WC_Abs_HALO_2227\\\\222735_Absorbance_03.txt',\n",
       " '../HALO/Data/CDOM/WC_Abs_HALO_2227\\\\222736_Absorbance_01.txt',\n",
       " '../HALO/Data/CDOM/WC_Abs_HALO_2227\\\\222736_Absorbance_02.txt',\n",
       " '../HALO/Data/CDOM/WC_Abs_HALO_2227\\\\222736_Absorbance_03.txt',\n",
       " '../HALO/Data/CDOM/WC_Abs_HALO_2227\\\\222737_Absorbance_01.txt',\n",
       " '../HALO/Data/CDOM/WC_Abs_HALO_2227\\\\222737_Absorbance_02.txt',\n",
       " '../HALO/Data/CDOM/WC_Abs_HALO_2227\\\\222737_Absorbance_03.txt',\n",
       " '../HALO/Data/CDOM/WC_Abs_HALO_2227\\\\222738_Absorbance_01.txt',\n",
       " '../HALO/Data/CDOM/WC_Abs_HALO_2227\\\\222738_Absorbance_02.txt',\n",
       " '../HALO/Data/CDOM/WC_Abs_HALO_2227\\\\222738_Absorbance_03.txt',\n",
       " '../HALO/Data/CDOM/WC_Abs_HALO_2227\\\\222740_Absorbance_01.txt',\n",
       " '../HALO/Data/CDOM/WC_Abs_HALO_2227\\\\222740_Absorbance_02.txt',\n",
       " '../HALO/Data/CDOM/WC_Abs_HALO_2227\\\\222740_Absorbance_03.txt']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#chose group of files \n",
    "files = glob.glob(f'../HALO/Data/CDOM/WC_Abs_HALO_2227/*.txt')\n",
    "\n",
    "#input info about run \n",
    "sample_type = 'WC'\n",
    "station = '2229'\n",
    "camp = 'HALO'\n",
    "Dil_fac = \"none\"\n",
    "\n",
    "files.sort() \n",
    "print('I have your files') \n",
    "\n",
    "save_name_1 = f'../HALO/Data/CDOM/Spectra_corrections_{sample_type}_{camp}_{station}.png'\n",
    "save_name_2 = f'../HALO/Data/CDOM/Spectra_transforms_{sample_type}_{camp}_{station}.png'\n",
    "\n",
    "save_name_3 = f'../HALO/Data/CDOM/spectra_{sample_type}_{camp}_{station}.csv' \n",
    "save_name_4 = f'../HALO/Data/CDOM/sprectra_baseline_corrected_{sample_type}_{camp}_{station}.csv' \n",
    "save_name_5 = f'../HALO/Data/CDOM/sprectra_pchip_baseline_corrected_{sample_type}_{camp}_{station}.csv' \n",
    "save_name_6 = f'../HALO/Data/CDOM/sprectra_pchip_{sample_type}_{camp}_{station}.csv'\n",
    "save_name_7 = f'../HALO/Data/CDOM/CDOM_outputs_{sample_type}_{camp}_{station}.csv'\n",
    "\n",
    "pathlength = .01\n",
    "psu = 0\n",
    "\n",
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4bc25c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "#chose dilution factor labs \n",
    "if Dil_fac == 'none': \n",
    "    dil_fac = 1\n",
    "elif Dil_fac == 'five':\n",
    "    dil_fac = 5\n",
    "elif Dil_fac == 'two':\n",
    "    dil_fac = 2\n",
    "elif Dil_fac == 'four':\n",
    "    dil_fac = 4\n",
    "elif Dil_fac == 'twenty':\n",
    "    dil_fac = 20\n",
    "elif Dil_fac == 'ten':\n",
    "    dil_fac = 10\n",
    "elif Dil_fac == 'forty':\n",
    "    dil_fac = 40\n",
    "elif Dil_fac == 'eighty':\n",
    "    dil_fac = 80\n",
    "else: \n",
    "    dil_fac = XXXX\n",
    "    print(\"ERROR: Dilution factor is not right\")\n",
    "\n",
    "#chose axis labs \n",
    "if sample_type == 'WC': \n",
    "    ylimmin = -10\n",
    "    ylimmax = 200\n",
    "    ylimmin2 = -2\n",
    "    ylimmax2 = 2\n",
    "elif sample_type == 'WC_PW': \n",
    "    ylimmin = -10\n",
    "    ylimmax = 200\n",
    "    ylimmin2 = -2\n",
    "    ylimmax2 = 2\n",
    "elif sample_type == 'WC_RZ': \n",
    "    ylimmin = -10\n",
    "    ylimmax = 200\n",
    "    ylimmin2 = -2\n",
    "    ylimmax2 = 2\n",
    "elif sample_type == 'BC': \n",
    "    ylimmin = -1\n",
    "    ylimmax = 20\n",
    "    ylimmin2 = -.2\n",
    "    ylimmax2 = .2\n",
    "elif sample_type == 'PW': \n",
    "    ylimmin = -10\n",
    "    ylimmax = 200\n",
    "    ylimmin2 = -2\n",
    "    ylimmax2 = 2\n",
    "elif sample_type == 'RZ': \n",
    "    ylimmin = -10\n",
    "    ylimmax = 200\n",
    "    ylimmin2 = -2\n",
    "    ylimmax2 = 2\n",
    "else: \n",
    "    pathlength = XXXX\n",
    "    print(\"ERROR: pathlength is not right\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "89fa6718",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spectrometer ID: FLMS12623\n"
     ]
    }
   ],
   "source": [
    "# Replace 'your_file.txt' with the actual path to your text file\n",
    "with open(files[0], 'r') as file:\n",
    "    lines = file.readlines()\n",
    "# Join the lines into a single string\n",
    "header = ''.join(lines)\n",
    "\n",
    "# Search for the line that starts with \"Spectrometer: \" and capture the ID that follows\n",
    "match = re.search(r'Spectrometer:\\s+(\\w+)', header)\n",
    "\n",
    "# If a match is found, print and save it\n",
    "if match:\n",
    "    spectrometer_id = match.group(1)\n",
    "    print(f\"Spectrometer ID: {spectrometer_id}\")\n",
    "else:\n",
    "    print(\"No match found.\")\n",
    "\n",
    "if spectrometer_id == 'FLMS02459': \n",
    "    spec_used = \"CDOM\"\n",
    "elif spectrometer_id == \"FLMS12623\": \n",
    "    spec_used = \"Flame\"\n",
    "else: \n",
    "    print(\"no spec found\")\n",
    "\n",
    "if spec_used == \"Flame\": \n",
    "    wl_range = 751\n",
    "    base_cor_min = 700 \n",
    "    base_cor_max = 725 \n",
    "elif spec_used == \"CDOM\": \n",
    "    wl_range = 523\n",
    "    base_cor_min = np.nan \n",
    "    base_cor_max = np.nan \n",
    "else: \n",
    "    print(\"ERROR YOU DONT HAVE A SPEC FOR THIS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "98f7628e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pathlenth:  0.01\n",
      "Wavelenth max:  751\n",
      "Dilution Factor:  1\n",
      "Y limet maximun:  200\n"
     ]
    }
   ],
   "source": [
    "#MANUAL OVERRIDE    \n",
    "#pathlength = .50\n",
    "#psu = 35X\n",
    "#dil_fac = XX\n",
    "\n",
    "print('Pathlenth: ', pathlength)\n",
    "print('Wavelenth max: ', wl_range)\n",
    "#print('Salinity: ', psu)\n",
    "print('Dilution Factor: ', dil_fac)\n",
    "print('Y limet maximun: ',ylimmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "55918520",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 39\u001b[0m\n\u001b[0;32m     37\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124malt used\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m: \n\u001b[1;32m---> 39\u001b[0m     frames \u001b[38;5;241m=\u001b[39m [open_files(f) \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m files]\n\u001b[0;32m     40\u001b[0m     df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat(frames, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;66;03m# joins along cols  \u001b[39;00m\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfiles loaded and the data frame is made\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[6], line 39\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     37\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124malt used\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m: \n\u001b[1;32m---> 39\u001b[0m     frames \u001b[38;5;241m=\u001b[39m [open_files(f) \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m files]\n\u001b[0;32m     40\u001b[0m     df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat(frames, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;66;03m# joins along cols  \u001b[39;00m\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfiles loaded and the data frame is made\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[6], line 6\u001b[0m, in \u001b[0;36mopen_files\u001b[1;34m(name)\u001b[0m\n\u001b[0;32m      4\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_table(name, skiprows\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m14\u001b[39m,names\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwl\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mabs\u001b[39m\u001b[38;5;124m'\u001b[39m], index_col\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwl\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# extract ID from filename\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m ID \u001b[38;5;241m=\u001b[39m name[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m4\u001b[39m]\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m2\u001b[39m]\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# rename col name with ID\u001b[39;00m\n\u001b[0;32m      8\u001b[0m df \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mrename(columns\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mabs\u001b[39m\u001b[38;5;124m'\u001b[39m: ID})\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "#LOAD FILE\n",
    "def open_files(name):\n",
    "    # reads individual scan files\n",
    "    df = pd.read_table(name, skiprows=14,names=['wl', 'abs'], index_col='wl')\n",
    "    # extract ID from filename\n",
    "    ID = name[:-4].split(\"\\\\\")[2]\n",
    "    # rename col name with ID\n",
    "    df = df.rename(columns={'abs': ID})\n",
    "    return df\n",
    "\n",
    "def open_files_alt(name):\n",
    "    # reads individual scan files\n",
    "    with open(name, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    wl_line = lines[14].strip() \n",
    "    abs_ion_line = lines[15].strip()\n",
    "    wl_names = wl_line.split('\\t')\n",
    "    abs_ion_names = abs_ion_line.split('\\t')\n",
    "    df = pd.read_table(name, skiprows=16, names=['wl', 'abs'])\n",
    "    df['wl'] = wl_names\n",
    "    df['abs'] = abs_ion_names[2:]\n",
    "    df.index = df['wl']\n",
    "    df = df.drop('wl', axis = 1)\n",
    "    ID = name[:-4].split(\"\\\\\")[1]\n",
    "    # rename col name with ID\n",
    "    df = df.rename(columns={'abs': ID})\n",
    "    return df\n",
    "\n",
    "#LOOP TRU FILES TO MAKE ONE EASY TO USE DATA FRAME \n",
    "#columns are scanns and rows are wavelths \n",
    "\n",
    "if station == \"St.14\" and sample_type == \"WC\": \n",
    "    frames = [open_files_alt(f) for f in files]\n",
    "    df = pd.concat(frames, axis=1) # joins along cols  \n",
    "    df = df.apply(pd.to_numeric, errors='coerce')  # This will convert non-numeric values to NaN\n",
    "    df.index = pd.to_numeric(df.index, errors='coerce')\n",
    "    print(\"alt used\")\n",
    "else: \n",
    "    frames = [open_files(f) for f in files]\n",
    "    df = pd.concat(frames, axis=1) # joins along cols  \n",
    "\n",
    "print(\"files loaded and the data frame is made\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e781ea38",
   "metadata": {},
   "source": [
    "## PART 2: CORRECTING THE DATA \n",
    "\n",
    "- Absorbace to absorption \n",
    "\n",
    "- Salinity coorection \n",
    "\n",
    "- Temprature correction "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04095eac",
   "metadata": {},
   "source": [
    "### Absorbance to absortion\n",
    "\n",
    "absortion = absorbance*(2.303/pathlength)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98609b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Absorbance to Absortion\n",
    "df_cl = df*(2.303/pathlength) # to avoid overwriting when reruning the cell"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9e37f3c",
   "metadata": {},
   "source": [
    "### Correct it for Dilition \n",
    "\n",
    "absortion = absortion*dilution factor "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "313a8b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Absorbance to Absortion\n",
    "df_dil = df_cl*(dil_fac) # to avoid overwriting when reruning the cell\n",
    "df_dil"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc47a9c2",
   "metadata": {},
   "source": [
    "### Salinity correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2273d1bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#HERE WE LOAD AND INTIGRATE THE CORRECTION VALUES \n",
    "\n",
    "#Load correction values \n",
    "df_c1 = df_dil\n",
    "corr = pd.read_csv('../CDOM_DATA/sal_temp_corr.csv')\n",
    "corr.rename(columns={'wl_nm':'wl'}, inplace=True)\n",
    "corr.set_index('wl', inplace=True)\n",
    "\n",
    "# Interpolate corr curve to measured wl \n",
    "x = corr.index # corr wl\n",
    "xi = df_c1[250:wl_range].index # measured wl, no need to overwrite\n",
    "\n",
    "#pchip temp  \n",
    "y = corr['psi_t'] \n",
    "pchip_t = PchipInterpolator(x, y, axis=1)\n",
    "psi_t = pchip_t(xi) \n",
    "\n",
    "#pchip salt \n",
    "y = corr['psi_s'] \n",
    "pchip_s = PchipInterpolator(x, y, axis=1)\n",
    "psi_s = pchip_s(xi)\n",
    "\n",
    "# to dataframe\n",
    "correction = pd.DataFrame({'psi_t': psi_t, 'psi_s':psi_s}, index=xi)\n",
    "\n",
    "# Plot\n",
    "fig, ax = plt.subplots(figsize=(7.5,5))\n",
    "corr.plot(ax=ax)\n",
    "correction.plot(ls=':',ax=ax)\n",
    "ax.set_ylim(-.01,.03)\n",
    "ax.set_xlim(250,wl_range)\n",
    "ax.set_xlabel(\"Wavelength\")\n",
    "ax.set_ylabel(\"a$_{CDOM}$ (1/m)\")\n",
    "#ax.get_legend().remove() \n",
    "\n",
    "if corr.mean().mean()/correction.mean().mean() > .8: \n",
    "    print('graph showing the interprolated correction')\n",
    "else:\n",
    "    print('SOMETHING IS WRONG!')\n",
    "    print('interprolated correction mean is less then 80%')\n",
    "    print((corr.mean().mean()/correction.mean().mean())*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9799e7c3",
   "metadata": {},
   "source": [
    "### Salinity correction\n",
    "from sullivan 2006 \n",
    "\n",
    "abs = abs_mesured - psu*psi_s "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b801a619",
   "metadata": {},
   "outputs": [],
   "source": [
    "#HERE WE CORRECT FOR SALINITY \n",
    "#from sullivan 2006 \n",
    "\n",
    "#a = a_m - psu*psi_s \n",
    "\n",
    "nacl = 31.56\n",
    "\n",
    "Frames = []\n",
    "for columnName in df_dil:\n",
    "    Sample_ID = re.findall(r'\\d{6}', columnName)\n",
    "    Sample_ID = int(Sample_ID[0]) if Sample_ID else None\n",
    "    selected_data = df_dil.loc[250:wl_range, columnName]\n",
    "    col_cs = selected_data.sub(psu*correction['psi_s'], axis=0)\n",
    "    Frames.append(col_cs)\n",
    "\n",
    "df_cs = pd.concat(Frames, axis=1, ignore_index=False)\n",
    "New_Labels = df_dil.columns\n",
    "df_cs.columns = New_Labels\n",
    "\n",
    "sal_corr_per = ((df_dil.mean().mean()-df_dil.mean().mean())/(df_cs.mean().mean()+df_dil.mean().mean())/2)*100\n",
    "\n",
    "print(\"Min val of clean data:\",df_cl.min().min()) \n",
    "print(\"Min val of dilution data:\",df_dil.min().min()) \n",
    "print(\"Min val of salinity Corr:\",df_cs.min().min()) \n",
    "print(sal_corr_per, \"% differance in mean values for dilution to sal corrected\")\n",
    "\n",
    "if df_cs.mean().mean()/df_dil.mean().mean() > .8: \n",
    "    print('graph showing salinity corrected data')\n",
    "else:\n",
    "    print('SOMETHING IS WRONG!')\n",
    "    print('interprolated correction mean is less then 80%')\n",
    "    print((df_cs.mean().mean()/df_cl.mean().mean())*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfa84e62",
   "metadata": {},
   "source": [
    "### Temperature correction\n",
    "from sullivan 2006 \n",
    "\n",
    "abs = abs_mesured - temp*psi_t "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce91d9c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#HERE WE DEFINE THE END AND DEFINE THE FUNTIONS \n",
    "\n",
    "#define the end section \n",
    "df_cs = df_dil \n",
    "df_700 = df_cs[700:750] # df_cs is clean, abs and sal corrct aplied\n",
    "temp_cor_700 = correction[700:750]['psi_t']       \n",
    "\n",
    "#define temp correction \n",
    "def temp_cor(temp,\n",
    "             c_abs, # the column\n",
    "             t_cor= correction['psi_t']# the correction\n",
    "            ):\n",
    "    return (c_abs-t_cor*temp)\n",
    "\n",
    "# this is the function to minimize the root mean squared \n",
    "def rmse(temp, col_abs):\n",
    "    cor = temp_cor(temp, c_abs=col_abs, t_cor=correction[700:750]['psi_t'])\n",
    "    col_abs_mean = col_abs.mean()\n",
    "    col_resid = cor-col_abs_mean\n",
    "    diff = np.sqrt(np.mean(col_resid**2))\n",
    "    return  diff\n",
    "\n",
    "print('funtions are defined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aca8649",
   "metadata": {},
   "outputs": [],
   "source": [
    "#HERE WE DO THE TEMP CORRECTION FOR ALL THE DATA \n",
    "# minimization\n",
    "#maybe play with bonds for improvement\n",
    "#minimize(rmse, temp0, df_700.iloc[:,0], bounds=(min,max))\n",
    "\n",
    "coln = np.arange(0, len(df_700.columns), 1, dtype=int)\n",
    "Frames = []\n",
    "frames = []\n",
    "\n",
    "for col in coln:\n",
    "    res = minimize(rmse, 0, df_700.iloc[:,col])\n",
    "    col_ct = temp_cor(res.x, df_cs.iloc[:,col])\n",
    "    Frames.append(col_ct)\n",
    "    temp_val = res.x[0]\n",
    "    col_ct_resid = col_ct[700:750]-col_ct[700:750].mean()\n",
    "    temp_rmse = np.sqrt(np.mean(col_ct_resid**2))\n",
    "    dict = {'temp_val':[temp_val], \n",
    "           'temp_rmse':[temp_rmse]}\n",
    "    diff = pd.DataFrame(dict)\n",
    "    frames.append(diff) #save the inportant outputs \n",
    "\n",
    "df_ct = pd.concat(Frames, axis=1, ignore_index=False)\n",
    "New_Labels = df_cs.columns\n",
    "df_ct.columns = New_Labels\n",
    "\n",
    "outputs = pd.concat(frames, axis=0, ignore_index=False)\n",
    "New_Labels = df_cs.columns\n",
    "outputs.index = list(New_Labels)\n",
    "\n",
    "print(\"\")\n",
    "temp_corr_per = ((df_cs.mean().mean()-df_ct.mean().mean())/(df_cs.mean().mean()+df_ct.mean().mean())/2)*100\n",
    "print(temp_corr_per, \"% differance in mean values\")\n",
    "print(\"\")\n",
    "\n",
    "temp_cuttoff = 30\n",
    "max_temp = outputs[\"temp_val\"].max()\n",
    "temp_cout = abs(outputs[\"temp_val\"]).gt(30).sum()\n",
    "rmse_cutoff = 10\n",
    "max_rmse = outputs[\"temp_rmse\"].max()\n",
    "rmse_cout = outputs[\"temp_rmse\"].gt(rmse_cutoff).sum()\n",
    "\n",
    "if max_rmse > rmse_cutoff: \n",
    "    print(\"WARNING\")\n",
    "    print(\"RMSE PROBLEM\")\n",
    "    print(\"\")\n",
    "    print(\"max rmse:\", max_rmse)\n",
    "    print(\"rmse cuttoff:\", rmse_cutoff)\n",
    "    print(rmse_cout, 'fits were above the cuttoff')\n",
    "    print(\"Min val of sal Corr:\",df_cs.min().min()) \n",
    "    print(\"Min val of temo Corr:\",df_ct.min().min()) \n",
    "if max_temp > temp_cuttoff: \n",
    "    print(\"WARNING\")\n",
    "    print(\"TEMP PROBLEM\")\n",
    "    print(\"\")\n",
    "    print(\"max temp:\", max_temp)\n",
    "    print(\"temp cuttoff:\", temp_cuttoff)\n",
    "    print(temp_cout, 'fits were above the cuttoff')\n",
    "    print(\"Min val of sal Corr:\",df_cs.min().min()) \n",
    "    print(\"Min val of temo Corr:\",df_ct.min().min()) \n",
    "else: \n",
    "    print('the run was a success! :) ')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "787d407a",
   "metadata": {},
   "source": [
    "### baceline correction\n",
    "\n",
    "just subtract the mean from 700 to 725"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ae986ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#HERE WE DO THE BASLINE CORRECTION AND SAVE THE CORECTION AS 'bcorr'\n",
    "df_cs = df_dil\n",
    "df_ct = df_dil\n",
    "bcorr = df_ct[base_cor_min:base_cor_max].mean(axis=0) # 486:537 is 675 to 725 nm and this line takes the average \n",
    "df_bc = df_ct[:] - bcorr # here we correct the data by that average\n",
    "\n",
    "outputs['mean_700_725'] = bcorr\n",
    "print(\"\")\n",
    "print(\"mean value for the baceline:\", outputs['mean_700_725'].mean())\n",
    "print(\"min value for the baceline:\", outputs['mean_700_725'].min())\n",
    "print(\"max value for the baceline:\", outputs['mean_700_725'].max())\n",
    "print(\"\")\n",
    "print(\"Succcess\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46082eba",
   "metadata": {},
   "source": [
    "### Compare the data \n",
    "\n",
    "look at the graphs to see how the corrections changed the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95f77865",
   "metadata": {},
   "outputs": [],
   "source": [
    "#graph the diffrent transforms\n",
    "\n",
    "fig, axs = plt.subplots(nrows=3, ncols=2, figsize=(10, 8)) \n",
    "\n",
    "df.plot(ax=axs[0,0])\n",
    "df_cl.plot(ax=axs[0,1])\n",
    "df_dil.plot(ax=axs[1,0])\n",
    "df_cs.plot(ax=axs[1,1])\n",
    "df_ct.plot(ax=axs[2,0])\n",
    "df_bc.plot(ax=axs[2,1])\n",
    "\n",
    "axs[0,0].set_title(\"Absorption Spectra RAW\")\n",
    "axs[0,1].set_title(\"CDOM Spectra RAW\")\n",
    "axs[1,0].set_title(f\"Dilution Corrected:{dil_fac}X\")\n",
    "axs[1,1].set_title(\"Salinity Corrected\")\n",
    "axs[2,0].set_title(\"Temprature Corrected\")\n",
    "axs[2,1].set_title(\"Basline Adjusted\")\n",
    "\n",
    "axs[0,0].set_ylim(ylimmin,ylimmax)\n",
    "axs[0,1].set_ylim(ylimmin,ylimmax)\n",
    "axs[1,1].set_ylim(ylimmin,ylimmax)\n",
    "axs[1,0].set_ylim(ylimmin,ylimmax)\n",
    "axs[2,1].set_ylim(ylimmin,ylimmax)\n",
    "axs[2,0].set_ylim(ylimmin,ylimmax)\n",
    "\n",
    "axs[0,0].set_xlim(250,wl_range)\n",
    "axs[0,1].set_xlim(250,wl_range)\n",
    "axs[1,1].set_xlim(250,wl_range)\n",
    "axs[1,0].set_xlim(250,wl_range)\n",
    "axs[2,1].set_xlim(250,wl_range)\n",
    "axs[2,0].set_xlim(250,wl_range)\n",
    "\n",
    "axs[2,0].set_xlabel(\"Wavelength\")\n",
    "axs[2,1].set_xlabel(\"Wavelength\")\n",
    "axs[1,0].set_xlabel(\"\")\n",
    "axs[1,1].set_xlabel(\"\")\n",
    "axs[0,0].set_xlabel(\"\")\n",
    "axs[0,1].set_xlabel(\"\")\n",
    "\n",
    "axs[0,0].set_ylabel(\"a$_{CDOM}$ (1/m)\")\n",
    "axs[1,0].set_ylabel(\"a$_{CDOM}$ (1/m)\")\n",
    "axs[2,0].set_ylabel(\"a$_{CDOM}$ (1/m)\")\n",
    "\n",
    "\n",
    "axs[0,1].set_ylabel(\"\")\n",
    "axs[1,1].set_ylabel(\"\")\n",
    "axs[2,1].set_ylabel(\"\")\n",
    "\n",
    "\n",
    "axs[0,0].legend(\"\")\n",
    "axs[0,1].legend(\"\")\n",
    "axs[1,1].legend(\"\")\n",
    "axs[1,0].legend(\"\")\n",
    "axs[2,0].legend(\"\")\n",
    "axs[2,1].legend(\"\")\n",
    "\n",
    "\n",
    "fig.suptitle(f'Correction Visualization {sample_type} {camp} {station}', size = 15)\n",
    "fig.set_facecolor('w')\n",
    "plt.tight_layout()\n",
    "\n",
    "#save plot \n",
    "plt.savefig(save_name_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e9a9c8b",
   "metadata": {},
   "source": [
    "### PART 2: REGRESSIONS\n",
    "\n",
    "I need an output table for each sampling event with the sample number\n",
    "\n",
    "we will get this with 2 regression methods \n",
    "\n",
    "there are two ways we are going to calculate the spectral slopes \n",
    "1. Single exponential (SEM) where: $A(g)=Ae^{slope}$\n",
    "2. Hyperbolic (HM) where: $A(g)=A(\\frac{wl}{532})^{slope}$\n",
    "2. Power Law (PL) where:\n",
    "\n",
    "corresponding parameters - slopes \n",
    "                the spectral slope of 275 to 295\n",
    "                the spectral slope of 350 to 400\n",
    "                the spectral slope of 275 to 700\n",
    "\n",
    "corresponding parameters - slop ratios\n",
    "                spectral ratio of slope 275 to 295 to slope of 350 to 400\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "208c4b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DEFINE EXPONENTAL REGRESSION \n",
    "#A(g)=Ae^slope\n",
    "def exp_equation(x, a, s, b):\n",
    "    return a * np.exp(-s * x) + b\n",
    "\n",
    "#DEFINE HYPERBOLIC REGRESSION \n",
    "#A(g)=A(wl/532)^slope\n",
    "def hyp_equation(x, a, s, b):\n",
    "    return a * ((x/532)**-s) + b\n",
    "\n",
    "#DEFINE POWER LAW REGRESSION \n",
    "#going to do that in the future \n",
    "\n",
    "#add a wl col so we can use it in the rags  \n",
    "#df_final[\"wl\"] = df_final.index\n",
    "\n",
    "print(\"the functions are set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d32d2216",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SELECT THE corrected data and the basline adjusted data\n",
    "\n",
    "#corrected \n",
    "df_final = df_ct\n",
    "df_final[\"wl\"] = df_final.index\n",
    "\n",
    "#basline adjusted\n",
    "df_final_bc = df_bc\n",
    "df_final_bc[\"wl\"] = df_final_bc.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93b65823",
   "metadata": {},
   "outputs": [],
   "source": [
    "#HERE DO THE EXPONENTAL REGRESSION FOR ALL SAMPLES \n",
    "#Corrected Data \n",
    "\n",
    "Frames = []\n",
    "maxfev = 10000000\n",
    "for (columnName, columnData) in df_final.items():\n",
    "    col = columnName\n",
    "    #get the x and y values for all the slope intervals we want \n",
    "    ydata_s700 = df_final[(df_final['wl']>=275) & (df_final['wl']<=700)][col]\n",
    "    xdata_s700 = df_final[(df_final['wl']>=275) & (df_final['wl']<=700)]['wl']\n",
    "    ydata_s700 = np.asarray(ydata_s700, dtype=float).ravel()\n",
    "    xdata_s700 = np.asarray(xdata_s700, dtype=float).ravel()\n",
    "    ydata_s295 = df_final[(df_final['wl']>=275) & (df_final['wl']<=295)][col]\n",
    "    xdata_s295 = df_final[(df_final['wl']>=275) & (df_final['wl']<=295)]['wl']\n",
    "    ydata_s295 = np.asarray(ydata_s295, dtype=float).ravel()\n",
    "    xdata_s295 = np.asarray(xdata_s295, dtype=float).ravel()\n",
    "    ydata_s350 = df_final[(df_final['wl']>=350) & (df_final['wl']<=400)][col]\n",
    "    xdata_s350 = df_final[(df_final['wl']>=350) & (df_final['wl']<=400)]['wl']\n",
    "    ydata_s350 = np.asarray(ydata_s350, dtype=float).ravel()\n",
    "    xdata_s350 = np.asarray(xdata_s350, dtype=float).ravel()\n",
    "    \n",
    "    #set some innital guesses to the curve fit \n",
    "    pams_700 = (15, .001, 0)\n",
    "    pams_295 = (10, .001, 0)\n",
    "    pams_350 = (10, .001, 0)\n",
    "    \n",
    "    #regression exp_equation 275 to 700\n",
    "    params, cv = curve_fit(exp_equation, xdata_s700, ydata_s700, p0=pams_700, maxfev=maxfev)\n",
    "    a, s, b = params\n",
    "    \n",
    "    #get the valuse for exp_equation 275 to 700 and calcuate rSquared\n",
    "    squaredDiffs = np.square(ydata_s700 - exp_equation(xdata_s700, a, s, b))\n",
    "    squaredDiffsFromMean = np.square(ydata_s700 - np.mean(ydata_s700))\n",
    "    rSquared = 1 - np.sum(squaredDiffs) / np.sum(squaredDiffsFromMean)\n",
    "    R2_275_700 = rSquared\n",
    "    ss_275_700 = s\n",
    "    \n",
    "    #get the waveleths we need\n",
    "    abs_254 = exp_equation(252, a, s, b)\n",
    "    abs_280 = exp_equation(280, a, s, b)\n",
    "    abs_320 = exp_equation(320, a, s, b)\n",
    "    abs_350 = exp_equation(350, a, s, b)\n",
    "    abs_412 = exp_equation(412, a, s, b)\n",
    "    abs_440 = exp_equation(440, a, s, b)\n",
    "    \n",
    "    #regression exp_equation 275 to 295\n",
    "    params, cv = curve_fit(exp_equation, xdata_s295, ydata_s295, p0=pams_295, maxfev=maxfev)\n",
    "    a, s, b = params\n",
    "    \n",
    "    #get the valuse for exp_equation 270 to 295 and calcuate rSquared\n",
    "    squaredDiffs = np.square(ydata_s295 - exp_equation(xdata_s295, a, s, b))\n",
    "    squaredDiffsFromMean = np.square(ydata_s295 - np.mean(ydata_s295))\n",
    "    rSquared = 1 - np.sum(squaredDiffs) / np.sum(squaredDiffsFromMean)\n",
    "    R2_275_295 = rSquared\n",
    "    ss_275_295 = s\n",
    "    \n",
    "    #regression exp_equation 350 to 400\n",
    "    params, cv = curve_fit(exp_equation, xdata_s350, ydata_s350, p0=pams_350, maxfev=maxfev)\n",
    "    a, s, b = params\n",
    "    \n",
    "    #get the valuse for exp_equation 350 to 400 and calcuate rSquared\n",
    "    squaredDiffs = np.square(ydata_s350 - exp_equation(xdata_s350, a, s, b))\n",
    "    squaredDiffsFromMean = np.square(ydata_s350 - np.mean(ydata_s350))\n",
    "    rSquared = 1 - np.sum(squaredDiffs) / np.sum(squaredDiffsFromMean)\n",
    "    R2_350_400 = rSquared\n",
    "    ss_350_400 = s \n",
    "    \n",
    "    #get the spectral ratio \n",
    "    sr = ss_275_295 / ss_350_400\n",
    "    \n",
    "    dict = {'sample':[columnName],\n",
    "        'ss_275_700_sem':[ss_275_700],\n",
    "        'ss_275_295_sem':[ss_275_295],\n",
    "        'ss_350_400_sem':[ss_350_400],\n",
    "        'R2_275_700_sem':[R2_275_700],\n",
    "        'R2_275_295_sem':[R2_275_295],\n",
    "        'R2_350_400_sem':[R2_350_400],\n",
    "        'SR_sem':[sr]}\n",
    "    ss_outs = pd.DataFrame(dict)\n",
    "    ss_outs = ss_outs.set_index('sample', drop=True)\n",
    "    Frames.append(ss_outs)\n",
    "\n",
    "ss_outs_sem = pd.concat(Frames, axis=0, ignore_index=False) # adds all the things in the frame together to make a new df \n",
    "ss_outs_sem = ss_outs_sem.drop('wl')\n",
    "\n",
    "print(\"Single exponential done! yay!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eaac8ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#HERE DO THE HYPERBOLIC REGRESSION FOR ALL SAMPLES \n",
    "Frames = []\n",
    "maxfev = 10000000\n",
    "for (columnName, columnData) in df_final.items():\n",
    "    col = columnName\n",
    "    #get the x and y values for all the slope intervals we want \n",
    "    #get the x and y values for all the slope intervals we want \n",
    "    ydata_s700 = df_final[(df_final['wl']>=275) & (df_final['wl']<=700)][col]\n",
    "    xdata_s700 = df_final[(df_final['wl']>=275) & (df_final['wl']<=700)]['wl']\n",
    "    ydata_s700 = np.asarray(ydata_s700, dtype=float).ravel()\n",
    "    xdata_s700 = np.asarray(xdata_s700, dtype=float).ravel()\n",
    "    ydata_s295 = df_final[(df_final['wl']>=275) & (df_final['wl']<=295)][col]\n",
    "    xdata_s295 = df_final[(df_final['wl']>=275) & (df_final['wl']<=295)]['wl']\n",
    "    ydata_s295 = np.asarray(ydata_s295, dtype=float).ravel()\n",
    "    xdata_s295 = np.asarray(xdata_s295, dtype=float).ravel()\n",
    "    ydata_s350 = df_final[(df_final['wl']>=350) & (df_final['wl']<=400)][col]\n",
    "    xdata_s350 = df_final[(df_final['wl']>=350) & (df_final['wl']<=400)]['wl']\n",
    "    ydata_s350 = np.asarray(ydata_s350, dtype=float).ravel()\n",
    "    xdata_s350 = np.asarray(xdata_s350, dtype=float).ravel()\n",
    "    \n",
    "    #set some innital guesses to the curve fit \n",
    "    pams_700 = (.001, 11, 0)\n",
    "    pams_295 = (1, 0, 1)\n",
    "    pams_350 = (1, 1, 1)\n",
    "    \n",
    "    maxfev = 1000000\n",
    "    \n",
    "    #regression exp_equation 275 to 700\n",
    "    params, cv = curve_fit(hyp_equation, xdata_s700, ydata_s700, p0=pams_700, maxfev=maxfev)\n",
    "    a, s, b = params\n",
    "    \n",
    "    #print out for exp_equation 275 to 700 and calcuate rSquared\n",
    "    squaredDiffs = np.square(ydata_s700 - hyp_equation(xdata_s700, a, s, b))\n",
    "    squaredDiffsFromMean = np.square(ydata_s700 - np.mean(ydata_s700))\n",
    "    rSquared = 1 - np.sum(squaredDiffs) / np.sum(squaredDiffsFromMean)\n",
    "    R2_275_700 = rSquared\n",
    "    ss_275_700 = s\n",
    "    \n",
    "    #get the waveleths we need\n",
    "    abs_254 = hyp_equation(252, a, s, b)\n",
    "    abs_280 = hyp_equation(280, a, s, b)\n",
    "    abs_320 = hyp_equation(320, a, s, b)\n",
    "    abs_350 = hyp_equation(350, a, s, b)\n",
    "    abs_412 = hyp_equation(412, a, s, b)\n",
    "    abs_440 = hyp_equation(440, a, s, b)\n",
    "    \n",
    "    #regression exp_equation 275 to 295\n",
    "    params, cv = curve_fit(hyp_equation, xdata_s295, ydata_s295, p0=pams_295, maxfev=maxfev)\n",
    "    a, s, b = params\n",
    "    \n",
    "    #print out for exp_equation 270 to 295 and calcuate rSquared\n",
    "    squaredDiffs = np.square(ydata_s295 - hyp_equation(xdata_s295, a, s, b))\n",
    "    squaredDiffsFromMean = np.square(ydata_s295 - np.mean(ydata_s295))\n",
    "    rSquared = 1 - np.sum(squaredDiffs) / np.sum(squaredDiffsFromMean)\n",
    "    R2_275_295 = rSquared\n",
    "    ss_275_295 = s\n",
    "    \n",
    "    #regression exp_equation 350 to 400\n",
    "    params, cv = curve_fit(hyp_equation, xdata_s350, ydata_s350, p0=pams_350, maxfev=maxfev)\n",
    "    a, s, b = params\n",
    "    \n",
    "    #print out for exp_equation 350 to 400 and calcuate rSquared\n",
    "    squaredDiffs = np.square(ydata_s350 - hyp_equation(xdata_s350, a, s, b))\n",
    "    squaredDiffsFromMean = np.square(ydata_s350 - np.mean(ydata_s350))\n",
    "    rSquared = 1 - np.sum(squaredDiffs) / np.sum(squaredDiffsFromMean)\n",
    "    R2_350_400 = rSquared\n",
    "    ss_350_400 = s \n",
    "    \n",
    "    #get the spectral ratio \n",
    "    sr = ss_275_295 / ss_350_400\n",
    "    \n",
    "    dict = {'sample':[columnName],\n",
    "        'ss_275_700_hm':[ss_275_700],\n",
    "        'ss_275_295_hm':[ss_275_295],\n",
    "        'ss_350_400_hm':[ss_350_400],\n",
    "        'R2_275_700_hm':[R2_275_700],\n",
    "        'R2_275_295_hm':[R2_275_295],\n",
    "        'R2_350_400_hm':[R2_350_400],\n",
    "        'SR_hm':[sr]}\n",
    "    ss_outs = pd.DataFrame(dict)\n",
    "    ss_outs = ss_outs.set_index('sample', drop=True)\n",
    "    Frames.append(ss_outs)\n",
    "\n",
    "ss_outs_hm = pd.concat(Frames, axis=0, ignore_index=False) # adds all the things in the frame together to make a new df \n",
    "ss_outs_hm = ss_outs_hm.drop('wl')\n",
    "\n",
    "print(\"Hyperbolic done! yay!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc4247e8",
   "metadata": {},
   "source": [
    "#HERE DO THE HYPERBOLIC REGRESSION FOR ALL SAMPLES \n",
    "Frames = []\n",
    "maxfev = 10000000\n",
    "for (columnName, columnData) in df_final.iteritems():\n",
    "    col = columnName\n",
    "\n",
    "    R2_275_700 = np.nan\n",
    "    ss_275_700 = np.nan\n",
    "    \n",
    "    #get the waveleths we need\n",
    "    abs_254 = np.nan\n",
    "    abs_280 = np.nan\n",
    "    abs_320 = np.nan\n",
    "    abs_350 = np.nan\n",
    "    abs_412 = np.nan\n",
    "    abs_440 = np.nan\n",
    "    \n",
    "    R2_275_295 = np.nan\n",
    "    ss_275_295 = np.nan\n",
    "    \n",
    "    #print out for exp_equation 350 to 400 and calcuate rSquared\n",
    "    R2_350_400 = np.nan\n",
    "    ss_350_400 = np.nan\n",
    "    \n",
    "    #get the spectral ratio \n",
    "    sr = np.nan\n",
    "    \n",
    "    dict = {'sample':[columnName],\n",
    "        'ss_275_700_hm':[ss_275_700],\n",
    "        'ss_275_295_hm':[ss_275_295],\n",
    "        'ss_350_400_hm':[ss_350_400],\n",
    "        'R2_275_700_hm':[R2_275_700],\n",
    "        'R2_275_295_hm':[R2_275_295],\n",
    "        'R2_350_400_hm':[R2_350_400],\n",
    "        'SR_hm':[sr]}\n",
    "    ss_outs = pd.DataFrame(dict)\n",
    "    ss_outs = ss_outs.set_index('sample', drop=True)\n",
    "    Frames.append(ss_outs)\n",
    "\n",
    "ss_outs_hm = pd.concat(Frames, axis=0, ignore_index=False) # adds all the things in the frame together to make a new df \n",
    "ss_outs_hm = ss_outs_hm.drop('wl')\n",
    "\n",
    "print(\"Hyperbolic done! yay!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b206535",
   "metadata": {},
   "outputs": [],
   "source": [
    "#HERE WE TEST TO MAKE SURE THE R SQAURED IS GOOD \n",
    "#this will let us know if something is wrong and where\n",
    "\n",
    "#make the outputs an easy to use data frame \n",
    "ss_outs = pd.concat([ss_outs_sem, ss_outs_hm, outputs], axis = 1, ignore_index=False)\n",
    "\n",
    "#set the R2 level you want \n",
    "R_level = .8\n",
    "col_note_2 = f'R sqared cuttoff: {R_level}'\n",
    "col_note_2\n",
    "\n",
    "#print out the summary stats \n",
    "min_1 = ss_outs['R2_275_700_sem'].min()\n",
    "min_2 = ss_outs['R2_275_295_sem'].min()\n",
    "min_3 = ss_outs['R2_350_400_sem'].min()\n",
    "min_4 = ss_outs['R2_275_700_hm'].min()\n",
    "min_5 = ss_outs['R2_275_295_hm'].min()\n",
    "min_6 = ss_outs['R2_350_400_hm'].min()\n",
    "\n",
    "filter_col = [col for col in ss_outs if col.startswith('R2')]\n",
    "\n",
    "if 1 < 2:\n",
    "    print(\"\")\n",
    "    print(\"the R2 cuttoff was\", R_level)\n",
    "if min_1 < R_level: \n",
    "    print(\"\")\n",
    "    print(\"bad fits in Exponential 275 to 700:\", ss_outs[\"R2_275_700_sem\"].lt(R_level).sum())\n",
    "if min_2 < R_level:\n",
    "    print(\"\")\n",
    "    print(\"bad fits in Exponential 275 to 275:\", ss_outs[\"R2_275_295_sem\"].lt(R_level).sum())\n",
    "if min_3 < R_level:\n",
    "    print(\"\")\n",
    "    print(\"bad fits in Exponential 350 to 400:\", ss_outs[\"R2_350_400_sem\"].lt(R_level).sum())\n",
    "if min_4 < R_level:\n",
    "    print(\"\")\n",
    "    print(\"bad fits in Hyperbolic 275 to 700:\", ss_outs[\"R2_275_700_hm\"].lt(R_level).sum())\n",
    "if min_5 < R_level:\n",
    "    print(\"\")\n",
    "    print(\"bad fits in Hyperbolic 275 to 275:\", ss_outs[\"R2_275_295_hm\"].lt(R_level).sum())\n",
    "if min_6 < R_level:\n",
    "    print(\"\")\n",
    "    print(\"bad fits in Hyperbolic 350 to 400:\", ss_outs[\"R2_350_400_hm\"].lt(R_level).sum())\n",
    "if ss_outs[filter_col].gt(R_level).sum().sum() > ss_outs[filter_col].lt(R_level).sum().sum():\n",
    "    print(\"\")\n",
    "    print(ss_outs[filter_col].gt(R_level).sum().sum(),\": fits came out good\")\n",
    "    print(ss_outs[filter_col].lt(R_level).sum().sum(),\": fits came out bad\")\n",
    "    print(\"\")\n",
    "    print(\"Succsess:\", ss_outs[filter_col].gt(R_level).sum().sum()/ss_outs[filter_col].gt(-1000).sum().sum()*100,\"%\" )\n",
    "else: \n",
    "    print(\"\")\n",
    "    print(\"SOMETHING HAS GONE WRONG\")\n",
    "    print(\"\")\n",
    "    print(\"fits came out good\")\n",
    "    print(ss_outs[filter_col].gt(R_level).sum())\n",
    "    print(\"TOTAL GOOD:\", ss_outs[filter_col].gt(R_level).sum().sum())\n",
    "    print(\"\")\n",
    "    print(\"fits came out bad\")\n",
    "    print(ss_outs[filter_col].lt(R_level).sum())\n",
    "    print(\"TOTAL BAD:\", ss_outs[filter_col].lt(R_level).sum().sum())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb62c9d",
   "metadata": {},
   "source": [
    "### MAKE THE FINAL PRODUCTS \n",
    "\n",
    "corresponding parameters \n",
    "1. cdom abs at 254\n",
    "2. cdom abs at 320\n",
    "3. cdom abs at 350\n",
    "4. cdom abs at 412\n",
    "5. cdom abs at 440 \n",
    "6. cdom abs at 700"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93ce32cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#HERE WE MAKE A COPY INTERPROLATED DATA SET\n",
    "x = df_final.index\n",
    "coln = np.arange(0, len(df_final.columns), 1, dtype=int)\n",
    "\n",
    "Frames = []\n",
    "frames = []\n",
    "\n",
    "for col in coln:\n",
    "    y = df_final.iloc[:,col]\n",
    "    pchip_col = PchipInterpolator(x, y, axis=1)\n",
    "    xi = np.arange(250,wl_range,1)\n",
    "    yi = pchip_col(xi)\n",
    "    d = {'abs': yi}\n",
    "    pchip_c = pd.DataFrame(d, index = xi)\n",
    "    Frames.append(pchip_c)\n",
    "\n",
    "df_pchip = pd.concat(Frames, axis=1, ignore_index=False)\n",
    "New_Labels = df_final.columns\n",
    "df_pchip.columns = New_Labels\n",
    "print(\"the interproated data set for each scan is created\")\n",
    "\n",
    "#HERE WE MAKE A COPY OF THE DATA THAT IS VERTICAL SO JB LIKES IT\n",
    "abs_data_pchip = df_pchip.transpose(copy=False)\n",
    "abs_data = df_final.transpose(copy=False)\n",
    "print(\"the transpose data sets are created\")\n",
    "\n",
    "#LETS ADD THOSE IMPORTANT WAVELEGNTHS TO THE OUTOUTS TABEL \n",
    "abs_254 = df_pchip.iloc[4:5,]\n",
    "abs_254 = abs_254.transpose(copy=False)\n",
    "abs_320 = df_pchip.iloc[70:71,]\n",
    "abs_320 = abs_320.transpose(copy=False)\n",
    "abs_350 = df_pchip.iloc[100:101,]\n",
    "abs_350 = abs_350.transpose(copy=False)\n",
    "abs_412 = df_pchip.iloc[162:163,]\n",
    "abs_412 = abs_412.transpose(copy=False)\n",
    "abs_440 = df_pchip.iloc[190:191,]\n",
    "abs_440 = abs_440.transpose(copy=False)\n",
    "abs_700 = df_pchip.iloc[450:451,]\n",
    "abs_700 = abs_700.transpose(copy=False)\n",
    "\n",
    "abs_out = pd.concat([abs_254, abs_320, abs_350, abs_412, abs_440, abs_700], axis=1)\n",
    "abs_out = abs_out.add_prefix('acdom_')\n",
    "outputs_scan = pd.concat([ss_outs, abs_out], axis = 1)\n",
    "outputs_scan = outputs_scan.drop('wl')\n",
    "\n",
    "print(\"\")\n",
    "print(\"Yay! we have the final outputs for the corrected data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3314bcee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#HERE WE MAKE A COPY INTERPROLATED DATA SET\n",
    "if spec_used == \"Flame\": \n",
    "    x = df_final_bc.index\n",
    "    coln = np.arange(0, len(df_final_bc.columns), 1, dtype=int)\n",
    "\n",
    "    Frames = []\n",
    "    frames = []\n",
    "\n",
    "    for col in coln:\n",
    "        y = df_final_bc.iloc[:,col]\n",
    "        pchip_col = PchipInterpolator(x, y, axis=1)\n",
    "        xi = np.arange(250,wl_range,1)\n",
    "        yi = pchip_col(xi)\n",
    "        d = {'abs': yi}\n",
    "        pchip_c = pd.DataFrame(d, index = xi)\n",
    "        Frames.append(pchip_c)\n",
    "\n",
    "    df_pchip_bc = pd.concat(Frames, axis=1, ignore_index=False)\n",
    "    New_Labels = df_final_bc.columns\n",
    "    df_pchip_bc.columns = New_Labels\n",
    "    print(\"the interproated data set for each scan is created\")\n",
    "\n",
    "    #HERE WE MAKE A COPY OF THE DATA THAT IS VERTICAL SO JB LIKES IT\n",
    "    abs_data_pchip_bc = df_pchip_bc.transpose(copy=False)\n",
    "    abs_data_bc = df_final_bc.transpose(copy=False)\n",
    "    print(\"the transpose data sets are created\")\n",
    "\n",
    "    #LETS ADD THOSE IMPORTANT WAVELEGNTHS TO THE OUTOUTS TABEL \n",
    "    abs_254 = df_pchip.iloc[4:5,]\n",
    "    abs_254 = abs_254.transpose(copy=False)\n",
    "    abs_320 = df_pchip.iloc[70:71,]\n",
    "    abs_320 = abs_320.transpose(copy=False)\n",
    "    abs_350 = df_pchip.iloc[100:101,]\n",
    "    abs_350 = abs_350.transpose(copy=False)\n",
    "    abs_412 = df_pchip.iloc[162:163,]\n",
    "    abs_412 = abs_412.transpose(copy=False)\n",
    "    abs_440 = df_pchip.iloc[190:191,]\n",
    "    abs_440 = abs_440.transpose(copy=False)\n",
    "    abs_700 = df_pchip.iloc[450:451,]\n",
    "    abs_700 = abs_700.transpose(copy=False)\n",
    "\n",
    "    abs_out = pd.concat([abs_254, abs_320, abs_350, abs_412, abs_440, abs_700], axis=1)\n",
    "    abs_out = abs_out.add_prefix('acdom_')\n",
    "    abs_out = abs_out.add_suffix('_bc')\n",
    "    outputs_scan = pd.concat([outputs_scan, abs_out], axis = 1)\n",
    "    outputs_scan = outputs_scan.drop('wl')\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"Yay! we have the final outputs for the basline ajusted data\")\n",
    "else: \n",
    "    print(\"\")\n",
    "    print(\"No worries! this was the CDOM speck\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05a585b7",
   "metadata": {},
   "source": [
    "### Summary "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1295d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#take out the wavelenths \n",
    "if spec_used == \"Flame\": \n",
    "    df_final = df_final.drop('wl', axis=1)\n",
    "    df_final_bc = df_final_bc.drop('wl', axis=1)\n",
    "    df_pchip = df_pchip.drop('wl', axis=1)\n",
    "    df_pchip_bc = df_pchip_bc.drop('wl', axis=1)\n",
    "else: \n",
    "    df_final = df_final.drop('wl', axis=1)\n",
    "    df_pchip = df_pchip.drop('wl', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae300842",
   "metadata": {},
   "outputs": [],
   "source": [
    "#graph the diffrent transforms\n",
    "if spec_used == \"Flame\": \n",
    "    fig, axs = plt.subplots(nrows=2, ncols=2, figsize=(10, 8)) \n",
    "\n",
    "    df_final.plot(ax=axs[0,0])\n",
    "    df_final_bc.plot(ax=axs[1,0])\n",
    "    df_pchip.plot(ax=axs[0,1])\n",
    "    df_pchip_bc.plot(ax=axs[1,1])\n",
    "\n",
    "    axs[0,0].set_title(\"CDOM Spectra\")\n",
    "    axs[0,1].set_title(\"Interprolated Spectra\")\n",
    "    axs[1,0].set_title(\"Basline Adjusted\")\n",
    "    axs[1,1].set_title(\"Interprolated and Basline Adjusted\")\n",
    "    \n",
    "    axs[0,0].set_ylim(ylimmin,ylimmax)\n",
    "    axs[0,1].set_ylim(ylimmin,ylimmax)\n",
    "    axs[1,1].set_ylim(ylimmin,ylimmax)\n",
    "    axs[1,0].set_ylim(ylimmin,ylimmax)\n",
    "\n",
    "    axs[0,0].set_xlim(250,750)\n",
    "    axs[0,1].set_xlim(250,750)\n",
    "    axs[1,1].set_xlim(250,750)\n",
    "    axs[1,0].set_xlim(250,750)\n",
    "    \n",
    "    axs[1,0].set_xlabel(\"Wavelength\")\n",
    "    axs[1,1].set_xlabel(\"Wavelength\")\n",
    "\n",
    "    axs[0,0].set_xlabel(\"\")\n",
    "    axs[0,1].set_xlabel(\"\")\n",
    "\n",
    "    axs[0,0].set_ylabel(\"a$_{CDOM}$ (1/m)\")\n",
    "    axs[1,0].set_ylabel(\"a$_{CDOM}$ (1/m)\")\n",
    "\n",
    "    axs[0,1].set_ylabel(\"\")\n",
    "    axs[1,1].set_ylabel(\"\")\n",
    "\n",
    "    axs[0,0].legend(\"\")\n",
    "    axs[0,1].legend(\"\")\n",
    "    axs[1,1].legend(\"\")\n",
    "    axs[1,0].legend(\"\")\n",
    "\n",
    "    fig.suptitle(f'Transformations Visualization {sample_type} {camp} {station}', size = 15)\n",
    "    fig.set_facecolor('w')\n",
    "    plt.tight_layout()\n",
    "\n",
    "    #save plot \n",
    "    plt.savefig(save_name_2)\n",
    "\n",
    "else: \n",
    "    fig, axs = plt.subplots(nrows=1, ncols=2, figsize=(10, 4)) \n",
    "\n",
    "    df_final.plot(ax=axs[0])\n",
    "    df_pchip.plot(ax=axs[1])\n",
    "\n",
    "    axs[0].set_title(\"CDOM Spectra\")\n",
    "    axs[1].set_title(\"Interprolated Spectra\")\n",
    "    \n",
    "    axs[0].set_ylim(ylimmin,ylimmax)\n",
    "    axs[1].set_ylim(ylimmin,ylimmax)\n",
    "\n",
    "    axs[0].set_xlim(250,530)\n",
    "    axs[1].set_xlim(250,530)\n",
    "    \n",
    "    axs[0].set_xlabel(\"Wavelength\")\n",
    "    axs[1].set_xlabel(\"Wavelength\")\n",
    "\n",
    "    axs[0].set_ylabel(\"a$_{CDOM}$ (1/m)\")\n",
    "\n",
    "    axs[1].set_ylabel(\"\")\n",
    "\n",
    "    axs[0].legend(\"\")\n",
    "    axs[1].legend(\"\")\n",
    "\n",
    "    fig.suptitle(f'Transformations Visualization {sample_type} {camp} {station}', size = 15)\n",
    "    fig.set_facecolor('w')\n",
    "    plt.tight_layout()\n",
    "\n",
    "    #save plot \n",
    "    plt.savefig(save_name_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afc82495",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"HERE IS A DELIGHTFULL SUMMARY OF THE RUN\")\n",
    "print(\"\")\n",
    "print('interprolated correction for sal and temp mean',(corr.mean().mean()/correction.mean().mean())*100,'%')\n",
    "print(\"\")\n",
    "\n",
    "print(\"SAL CORRECTION\")\n",
    "print(\"the PSU value was:\", psu)\n",
    "print(\"Min val of clean data:\",df_cl.min().min()) \n",
    "print(\"Min val of salinity Corr:\",df_cs.min().min()) \n",
    "print(sal_corr_per, \"% differance in mean values for clean to sal corrected\")\n",
    "\n",
    "print(\"\")\n",
    "print(\"TEMP FIT MINIMIZE\")\n",
    "print(temp_corr_per, \"% differance in mean values for the temp optimication\")\n",
    "print(\"Min val of sal Corr:\",df_cs.min().min()) \n",
    "print(\"Min val of temo Corr:\",df_ct.min().min()) \n",
    "print(\"max rmse:\", max_rmse)\n",
    "print(\"rmse cuttoff:\", rmse_cutoff)\n",
    "print(rmse_cout, 'fits were above the rmse cuttoff')\n",
    "print(\"max temp:\", max_temp)\n",
    "print(\"temp cuttoff:\", temp_cuttoff)\n",
    "print(temp_cout, 'fits were above the temp cuttoff')\n",
    "\n",
    "if max_rmse > rmse_cutoff: \n",
    "    print(\"WARNING\")\n",
    "    print(\"RMSE PROBLEM\")\n",
    "if max_temp > temp_cuttoff: \n",
    "    print(\"WARNING\")\n",
    "    print(\"TEMP PROBLEM\")\n",
    "else: \n",
    "    print('')\n",
    "    \n",
    "print(\"BACELINE CORRECTION\")\n",
    "print(\"mean value for the baceline:\", outputs['mean_700_725'].mean())\n",
    "print(\"min value for the baceline:\", outputs['mean_700_725'].min())\n",
    "print(\"max value for the baceline:\", outputs['mean_700_725'].max())\n",
    "print(\"\")\n",
    "print(\"THE REGRESSIONS\")\n",
    "print(\"the R2 cuttoff was\", R_level)\n",
    "print(\"bad fits in Exponential 275 to 700:\", ss_outs[\"R2_275_700_sem\"].lt(R_level).sum())\n",
    "print(\"bad fits in Exponential 275 to 275:\", ss_outs[\"R2_275_295_sem\"].lt(R_level).sum())\n",
    "print(\"bad fits in Exponential 350 to 400:\", ss_outs[\"R2_350_400_sem\"].lt(R_level).sum())\n",
    "print(\"bad fits in Hyperbolic 275 to 700:\", ss_outs[\"R2_275_700_hm\"].lt(R_level).sum())\n",
    "print(\"bad fits in Hyperbolic 275 to 275:\", ss_outs[\"R2_275_295_hm\"].lt(R_level).sum())\n",
    "print(\"bad fits in Hyperbolic 350 to 400:\", ss_outs[\"R2_350_400_hm\"].lt(R_level).sum())\n",
    "print(\"fits that came out good:\",ss_outs[filter_col].gt(R_level).sum().sum())\n",
    "print(\"fits that came out bad:\",ss_outs[filter_col].lt(R_level).sum().sum())\n",
    "print(\"Succsess rate of:\", ss_outs[filter_col].gt(R_level).sum().sum()/ss_outs[filter_col].gt(-1000).sum().sum()*100,\"%\" )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d85e9a80",
   "metadata": {},
   "source": [
    "### EXPORT THE DATA\n",
    "\n",
    "all done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "358ae24e",
   "metadata": {},
   "outputs": [],
   "source": [
    "abs_data = abs_data.drop('wl', axis=0)\n",
    "abs_data['file_id'] = abs_data.index\n",
    "abs_data['Sample_ID'] = abs_data['file_id'].str.extract('(\\d{6})')\n",
    "abs_data.index = abs_data['Sample_ID']\n",
    "abs_data = abs_data.drop('Sample_ID', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f76448b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "if spec_used == \"Flame\": \n",
    "    abs_data_bc = abs_data_bc.drop('wl', axis=0)\n",
    "    abs_data_bc['file_id'] = abs_data_bc.index\n",
    "    abs_data_bc['Sample_ID'] = abs_data_bc['file_id'].str.extract('(\\d{6})')\n",
    "    abs_data_bc.index = abs_data_bc['Sample_ID']\n",
    "    abs_data_bc = abs_data_bc.drop('Sample_ID', axis=1)\n",
    "else: \n",
    "    print(\"no need to save the basline\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9e15216",
   "metadata": {},
   "outputs": [],
   "source": [
    "abs_data_pchip = abs_data_pchip.drop('wl', axis=0)\n",
    "abs_data_pchip['file_id'] = abs_data_pchip.index\n",
    "abs_data_pchip['Sample_ID'] = abs_data_pchip['file_id'].str.extract('(\\d{6})')\n",
    "abs_data_pchip.index = abs_data_pchip['Sample_ID']\n",
    "abs_data_pchip = abs_data_pchip.drop('Sample_ID', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87e44cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "if spec_used == \"Flame\": \n",
    "    abs_data_pchip_bc = abs_data_pchip_bc.drop('wl', axis=0)\n",
    "    abs_data_pchip_bc['file_id'] = abs_data_pchip_bc.index\n",
    "    abs_data_pchip_bc['Sample_ID'] = abs_data_pchip_bc['file_id'].str.extract('(\\d{6})')\n",
    "    abs_data_pchip_bc.index = abs_data_pchip_bc['Sample_ID']\n",
    "    abs_data_pchip_bc = abs_data_pchip_bc.drop('Sample_ID', axis=1)\n",
    "else: \n",
    "    print(\"no need to save the basline\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "287af491",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs_scan['file_id'] = outputs_scan.index\n",
    "outputs_scan['Sample_ID'] = outputs_scan['file_id'].str.extract('(\\d{6})')\n",
    "outputs_scan.index = outputs_scan['Sample_ID']\n",
    "outputs_scan = outputs_scan.drop('Sample_ID', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84291611",
   "metadata": {},
   "outputs": [],
   "source": [
    "#OK, LETS SAVE WHAT WE WANT \n",
    "#save the spectra \n",
    "\n",
    "abs_data.to_csv(save_name_3)\n",
    "\n",
    "if spec_used == \"Flame\": \n",
    "    #save the spectra basline corrected \n",
    "    abs_data_bc.to_csv(save_name_4)\n",
    "    #save the p chip spectra basline corrected \n",
    "    abs_data_pchip_bc.to_csv(save_name_5)\n",
    "else: \n",
    "    print(\"no need to save the basline\")\n",
    "\n",
    "#save the p chip spectra \n",
    "abs_data_pchip.to_csv(save_name_6)\n",
    "\n",
    "#save the outputs data \n",
    "outputs_scan.to_csv(save_name_7)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88c3482a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Sample Type: ', sample_type)\n",
    "print('Station:     ', station)\n",
    "print('')\n",
    "print('Pathlenth: ', pathlength)\n",
    "print('Wavelenth max: ', wl_range)\n",
    "print('Salinity: ', psu)\n",
    "print('Dilution Factor: ', dil_fac)\n",
    "print('Y limet maximun: ',ylimmax)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
